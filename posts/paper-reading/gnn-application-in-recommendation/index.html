<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>GNN Application in Recommendation | Hi-^_^</title><meta name=keywords content="GNN,Recommendation"><meta name=description content="Prelude GNN是最近几年中机器学习较新的一个发展方向，也是学术界和工业界研究的热点话题。相比于node2vec、PageRank，GNN具有更强大的学习能力，能够更好得感知图数据。
GNN的一个经典应用场景是推荐系统。像淘宝这样的电商平台，大量的用户和商品，自然而然地形成了一个复杂异构图，一个直观想法是，可以利用GNN学习用户的兴趣特征，根据用户的历史浏览推荐用户更感兴趣的商品。而且这种方法的推荐质量也更高。
Pinterest Let&rsquo;s begin with PinSage 1 GNN的开山制作——GCN 2把GNN训练用矩阵乘法来刻画，这样会导致一个问题，每一层每个点都要从他的邻居中读取feature (i.e. 全图训练)，导致GNN的复杂度非常高。
后来，一个点在他的部分邻居中读取feature（i.e. 图采样）就可以保证模型的准确度，这样就减低了GNN训练的成本。PinSage也是一个基于采样的GNN算法，是一个早期同时也非常经典的GNN算法。和GraphSage相比，最大的不同点在于用random-walk，采样的子图的质量会好一些。
比如上图就是一个两层的GNN, 在采样完成后我们会得到很多个子图（i.e. mini-batch）并喂给网络训练。每层网络都是如下图所示的图卷积操作，通过聚合特征（e.g. 取平均，求和）再通过一层神经网络得到一下层的输入。将多层图卷积stack到一起就构成了整个GNN，一般只有2-3层。
然而这和推荐系统有什么关系呢？在Pinterest中，有很多pin（类似淘宝的商品，在推荐系统中也称之为item），每个item会包含图片、文本信息，将图片和文本的embedding拼接到一起得到item的初始特征向量。在训练完GNN之后，通过推理得到每个item的嵌入向量，之后推荐系统就可以根据embedding向用户推荐新的item。
Recoommandation System at Pinterest 到推荐系统中，我们会认为每个item的embeddin向量是通过一个黑箱过程（i.e. PinSage），是提前已知的 3。我们的问题是，知道用户点击item历史序列$\mathcal{A}=\{a_1, a_2, &mldr;\}$，我们想从$\mathcal{P}=\{p_1, p_2, &mldr;\}$中预测下一个用户可能感兴趣的商品。
我们可以想到一些很直观的方法，比如历史的item求平均，还可以按时间给一个权重。但是这样的方法把用户的embedding限制在一个有限的向量里面，效果不是很好，因为一个用户兴趣面是是很广的，对不同种类的商品有不同的喜好。PinnerSage 3 会给用户维护一个不限大小的embedding，不同的种类的喜好都对应到一个嵌入向量。
然而还有另外一个问题——用户的历史序列会很长，计算的复杂度会非常大，影响推荐的系统的延迟。PinnerSage采用的方法是，通过聚类，每个商品都可以用一个类中心来表示。在推荐的时候，根据用户的action再给这些类中心一个重要性评分。最后用最重要的类（e.g. top 3）去预测下一个item。
总结一下PinnerSage的推荐过程分为3步：
用户历史动作序列聚类 得到类中心嵌入向量（Medoid based） 根据time decay计算类的重要度 找哪一个item最合适是一个索引过程，一般会通过cache加速查询。为了兼顾用户的长期兴趣和系统延迟，PinnerSage将任务分成两类，daily和online。在每天PinnerSage都会把用户的历史动作进行聚类的，得到类中心和重要度，这样可以保证和用户兴趣。而在online推荐的时候，只会对用最新的动作在已有的类上refine，降低推荐的延迟。
AliGraph 4 图数据规模的增大给GNN训练带来新的挑战，在工业界，图的规模会非常大，点和边往往是billion量级的。Ali坐拥最大的电商平台之一，大规模图神经网络既有真实的场景，同时也是一个亟须解决的问题。
AliGraph是ali的一个图神经网络框架。从系统的角度来看，AliGraph需要为上层用户提供应有的支撑，使得上层算法和应用开发可以仅仅关注自身的逻辑，而不用担心资源如何分配的问题。
面对超大规模的图，AliGraph通过parition把图分布式存在集群上，解决了存储的问题。同时通过维护cache，尽可能减少partition之间的通讯开销。同时AliGraph还需要实现常见的算子来支持上层GNN应用。AliGraph也进行了开源，GraphLearn 5，同时支持推理和训练任务。
GNN训练基本都是大同小异，而推理任务是部署和落地中主要workload，而且大部分的计算资源也是花在推理上。考虑到真实场景下，图一般是动态（e.g. 新用户的加入），GraphLearn后来增加了Dynamic Graph Service (DGS)来支持动态图的推理任务。
整个系统通过微服务的方式运行，每个模块被拆到subservices，模块之间都通过队列连接起来（e.g. kafka）。
美团广告推荐6 可以发现在和Pinterest的推荐系统其实在设计上有很多共同点。
美团在推荐任务中有两个观察点：数据样本稀疏，广告投放后点击量非常少；用户兴趣存在时空特点，在不同时间点和地点用户有不同的兴趣点。通过异构图构建、注意力机制等方法解决的推荐不准确的问题。
整个推荐也分成online任务，保证请求低延迟；以及offline任务，用来更新图节点embedding，学习用户的长期兴趣。而整个系统的部署方式和GraphLearn也有一些相似性。
Challenges still exist 在很多业务场景下，图并不是静态不变的而是呈现出一种时间特征，一些点在$t_0$时加入，之后在$t_1$时删除。但是时序图GNN（TGNN）仍然是一个open的问题 7。在近几年GNN的发展中，开放的标准数据集（e.g. OGB 8）有着巨大的贡献，不论是GNN算法还是GNN系统开发，都会将OGB作为数据集和其他baseline比较。然而目前TGNN并没有对应的标准数据集，这样我们很难去比较不同来自不同团队的工作。另一个方面，GNN的收敛性和表达能力是可以被证明的，但是TGNN还是一个相对较新的算法，并没有GNN成熟，缺少基础的理论支撑。
GNN虽然网络层数少，计算资源需求不大，但是图和特征需要占用大量的存储资源，这给GNN部署和落地带来了一些挑战 9。"><meta name=author content><link rel=canonical href=https://Anlarry.github.io/posts/paper-reading/gnn-application-in-recommendation/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.min.adc94f6a0b1f14123eb9f0e26177c35ba3040fa9e921e93741358a9d6c6a42e6.css integrity="sha256-rclPagsfFBI+ufDiYXfDW6MED6npIek3QTWKnWxqQuY=" rel="preload stylesheet" as=style><link rel=preload href=/icon/ai512.png as=image><link ref=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.3.1/styles/atom-one-dark.min.css><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.3.1/highlight.min.js></script>
<script>hljs.highlightAll()</script><link href='https://fonts.googleapis.com/css?family=Source Code Pro' rel=stylesheet><link rel=icon href=https://Anlarry.github.io/icon/ai512.png><link rel=icon type=image/png sizes=16x16 href=https://Anlarry.github.io/icon/ai16.png><link rel=icon type=image/png sizes=32x32 href=https://Anlarry.github.io/icon/ai32.png><link rel=apple-touch-icon href=https://Anlarry.github.io/apple-touch-icon.png><link rel=mask-icon href=https://Anlarry.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.111.3"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="GNN Application in Recommendation"><meta property="og:description" content="Prelude GNN是最近几年中机器学习较新的一个发展方向，也是学术界和工业界研究的热点话题。相比于node2vec、PageRank，GNN具有更强大的学习能力，能够更好得感知图数据。
GNN的一个经典应用场景是推荐系统。像淘宝这样的电商平台，大量的用户和商品，自然而然地形成了一个复杂异构图，一个直观想法是，可以利用GNN学习用户的兴趣特征，根据用户的历史浏览推荐用户更感兴趣的商品。而且这种方法的推荐质量也更高。
Pinterest Let&rsquo;s begin with PinSage 1 GNN的开山制作——GCN 2把GNN训练用矩阵乘法来刻画，这样会导致一个问题，每一层每个点都要从他的邻居中读取feature (i.e. 全图训练)，导致GNN的复杂度非常高。
后来，一个点在他的部分邻居中读取feature（i.e. 图采样）就可以保证模型的准确度，这样就减低了GNN训练的成本。PinSage也是一个基于采样的GNN算法，是一个早期同时也非常经典的GNN算法。和GraphSage相比，最大的不同点在于用random-walk，采样的子图的质量会好一些。
比如上图就是一个两层的GNN, 在采样完成后我们会得到很多个子图（i.e. mini-batch）并喂给网络训练。每层网络都是如下图所示的图卷积操作，通过聚合特征（e.g. 取平均，求和）再通过一层神经网络得到一下层的输入。将多层图卷积stack到一起就构成了整个GNN，一般只有2-3层。
然而这和推荐系统有什么关系呢？在Pinterest中，有很多pin（类似淘宝的商品，在推荐系统中也称之为item），每个item会包含图片、文本信息，将图片和文本的embedding拼接到一起得到item的初始特征向量。在训练完GNN之后，通过推理得到每个item的嵌入向量，之后推荐系统就可以根据embedding向用户推荐新的item。
Recoommandation System at Pinterest 到推荐系统中，我们会认为每个item的embeddin向量是通过一个黑箱过程（i.e. PinSage），是提前已知的 3。我们的问题是，知道用户点击item历史序列$\mathcal{A}=\{a_1, a_2, &mldr;\}$，我们想从$\mathcal{P}=\{p_1, p_2, &mldr;\}$中预测下一个用户可能感兴趣的商品。
我们可以想到一些很直观的方法，比如历史的item求平均，还可以按时间给一个权重。但是这样的方法把用户的embedding限制在一个有限的向量里面，效果不是很好，因为一个用户兴趣面是是很广的，对不同种类的商品有不同的喜好。PinnerSage 3 会给用户维护一个不限大小的embedding，不同的种类的喜好都对应到一个嵌入向量。
然而还有另外一个问题——用户的历史序列会很长，计算的复杂度会非常大，影响推荐的系统的延迟。PinnerSage采用的方法是，通过聚类，每个商品都可以用一个类中心来表示。在推荐的时候，根据用户的action再给这些类中心一个重要性评分。最后用最重要的类（e.g. top 3）去预测下一个item。
总结一下PinnerSage的推荐过程分为3步：
用户历史动作序列聚类 得到类中心嵌入向量（Medoid based） 根据time decay计算类的重要度 找哪一个item最合适是一个索引过程，一般会通过cache加速查询。为了兼顾用户的长期兴趣和系统延迟，PinnerSage将任务分成两类，daily和online。在每天PinnerSage都会把用户的历史动作进行聚类的，得到类中心和重要度，这样可以保证和用户兴趣。而在online推荐的时候，只会对用最新的动作在已有的类上refine，降低推荐的延迟。
AliGraph 4 图数据规模的增大给GNN训练带来新的挑战，在工业界，图的规模会非常大，点和边往往是billion量级的。Ali坐拥最大的电商平台之一，大规模图神经网络既有真实的场景，同时也是一个亟须解决的问题。
AliGraph是ali的一个图神经网络框架。从系统的角度来看，AliGraph需要为上层用户提供应有的支撑，使得上层算法和应用开发可以仅仅关注自身的逻辑，而不用担心资源如何分配的问题。
面对超大规模的图，AliGraph通过parition把图分布式存在集群上，解决了存储的问题。同时通过维护cache，尽可能减少partition之间的通讯开销。同时AliGraph还需要实现常见的算子来支持上层GNN应用。AliGraph也进行了开源，GraphLearn 5，同时支持推理和训练任务。
GNN训练基本都是大同小异，而推理任务是部署和落地中主要workload，而且大部分的计算资源也是花在推理上。考虑到真实场景下，图一般是动态（e.g. 新用户的加入），GraphLearn后来增加了Dynamic Graph Service (DGS)来支持动态图的推理任务。
整个系统通过微服务的方式运行，每个模块被拆到subservices，模块之间都通过队列连接起来（e.g. kafka）。
美团广告推荐6 可以发现在和Pinterest的推荐系统其实在设计上有很多共同点。
美团在推荐任务中有两个观察点：数据样本稀疏，广告投放后点击量非常少；用户兴趣存在时空特点，在不同时间点和地点用户有不同的兴趣点。通过异构图构建、注意力机制等方法解决的推荐不准确的问题。
整个推荐也分成online任务，保证请求低延迟；以及offline任务，用来更新图节点embedding，学习用户的长期兴趣。而整个系统的部署方式和GraphLearn也有一些相似性。
Challenges still exist 在很多业务场景下，图并不是静态不变的而是呈现出一种时间特征，一些点在$t_0$时加入，之后在$t_1$时删除。但是时序图GNN（TGNN）仍然是一个open的问题 7。在近几年GNN的发展中，开放的标准数据集（e.g. OGB 8）有着巨大的贡献，不论是GNN算法还是GNN系统开发，都会将OGB作为数据集和其他baseline比较。然而目前TGNN并没有对应的标准数据集，这样我们很难去比较不同来自不同团队的工作。另一个方面，GNN的收敛性和表达能力是可以被证明的，但是TGNN还是一个相对较新的算法，并没有GNN成熟，缺少基础的理论支撑。
GNN虽然网络层数少，计算资源需求不大，但是图和特征需要占用大量的存储资源，这给GNN部署和落地带来了一些挑战 9。"><meta property="og:type" content="article"><meta property="og:url" content="https://Anlarry.github.io/posts/paper-reading/gnn-application-in-recommendation/"><meta property="og:image" content="https://Anlarry.github.io/2023-03-17-19-45-23.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-03-18T00:00:00+00:00"><meta property="article:modified_time" content="2023-03-18T00:00:00+00:00"><meta property="og:site_name" content="Hi~, ^_^"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://Anlarry.github.io/2023-03-17-19-45-23.png"><meta name=twitter:title content="GNN Application in Recommendation"><meta name=twitter:description content="Prelude GNN是最近几年中机器学习较新的一个发展方向，也是学术界和工业界研究的热点话题。相比于node2vec、PageRank，GNN具有更强大的学习能力，能够更好得感知图数据。
GNN的一个经典应用场景是推荐系统。像淘宝这样的电商平台，大量的用户和商品，自然而然地形成了一个复杂异构图，一个直观想法是，可以利用GNN学习用户的兴趣特征，根据用户的历史浏览推荐用户更感兴趣的商品。而且这种方法的推荐质量也更高。
Pinterest Let&rsquo;s begin with PinSage 1 GNN的开山制作——GCN 2把GNN训练用矩阵乘法来刻画，这样会导致一个问题，每一层每个点都要从他的邻居中读取feature (i.e. 全图训练)，导致GNN的复杂度非常高。
后来，一个点在他的部分邻居中读取feature（i.e. 图采样）就可以保证模型的准确度，这样就减低了GNN训练的成本。PinSage也是一个基于采样的GNN算法，是一个早期同时也非常经典的GNN算法。和GraphSage相比，最大的不同点在于用random-walk，采样的子图的质量会好一些。
比如上图就是一个两层的GNN, 在采样完成后我们会得到很多个子图（i.e. mini-batch）并喂给网络训练。每层网络都是如下图所示的图卷积操作，通过聚合特征（e.g. 取平均，求和）再通过一层神经网络得到一下层的输入。将多层图卷积stack到一起就构成了整个GNN，一般只有2-3层。
然而这和推荐系统有什么关系呢？在Pinterest中，有很多pin（类似淘宝的商品，在推荐系统中也称之为item），每个item会包含图片、文本信息，将图片和文本的embedding拼接到一起得到item的初始特征向量。在训练完GNN之后，通过推理得到每个item的嵌入向量，之后推荐系统就可以根据embedding向用户推荐新的item。
Recoommandation System at Pinterest 到推荐系统中，我们会认为每个item的embeddin向量是通过一个黑箱过程（i.e. PinSage），是提前已知的 3。我们的问题是，知道用户点击item历史序列$\mathcal{A}=\{a_1, a_2, &mldr;\}$，我们想从$\mathcal{P}=\{p_1, p_2, &mldr;\}$中预测下一个用户可能感兴趣的商品。
我们可以想到一些很直观的方法，比如历史的item求平均，还可以按时间给一个权重。但是这样的方法把用户的embedding限制在一个有限的向量里面，效果不是很好，因为一个用户兴趣面是是很广的，对不同种类的商品有不同的喜好。PinnerSage 3 会给用户维护一个不限大小的embedding，不同的种类的喜好都对应到一个嵌入向量。
然而还有另外一个问题——用户的历史序列会很长，计算的复杂度会非常大，影响推荐的系统的延迟。PinnerSage采用的方法是，通过聚类，每个商品都可以用一个类中心来表示。在推荐的时候，根据用户的action再给这些类中心一个重要性评分。最后用最重要的类（e.g. top 3）去预测下一个item。
总结一下PinnerSage的推荐过程分为3步：
用户历史动作序列聚类 得到类中心嵌入向量（Medoid based） 根据time decay计算类的重要度 找哪一个item最合适是一个索引过程，一般会通过cache加速查询。为了兼顾用户的长期兴趣和系统延迟，PinnerSage将任务分成两类，daily和online。在每天PinnerSage都会把用户的历史动作进行聚类的，得到类中心和重要度，这样可以保证和用户兴趣。而在online推荐的时候，只会对用最新的动作在已有的类上refine，降低推荐的延迟。
AliGraph 4 图数据规模的增大给GNN训练带来新的挑战，在工业界，图的规模会非常大，点和边往往是billion量级的。Ali坐拥最大的电商平台之一，大规模图神经网络既有真实的场景，同时也是一个亟须解决的问题。
AliGraph是ali的一个图神经网络框架。从系统的角度来看，AliGraph需要为上层用户提供应有的支撑，使得上层算法和应用开发可以仅仅关注自身的逻辑，而不用担心资源如何分配的问题。
面对超大规模的图，AliGraph通过parition把图分布式存在集群上，解决了存储的问题。同时通过维护cache，尽可能减少partition之间的通讯开销。同时AliGraph还需要实现常见的算子来支持上层GNN应用。AliGraph也进行了开源，GraphLearn 5，同时支持推理和训练任务。
GNN训练基本都是大同小异，而推理任务是部署和落地中主要workload，而且大部分的计算资源也是花在推理上。考虑到真实场景下，图一般是动态（e.g. 新用户的加入），GraphLearn后来增加了Dynamic Graph Service (DGS)来支持动态图的推理任务。
整个系统通过微服务的方式运行，每个模块被拆到subservices，模块之间都通过队列连接起来（e.g. kafka）。
美团广告推荐6 可以发现在和Pinterest的推荐系统其实在设计上有很多共同点。
美团在推荐任务中有两个观察点：数据样本稀疏，广告投放后点击量非常少；用户兴趣存在时空特点，在不同时间点和地点用户有不同的兴趣点。通过异构图构建、注意力机制等方法解决的推荐不准确的问题。
整个推荐也分成online任务，保证请求低延迟；以及offline任务，用来更新图节点embedding，学习用户的长期兴趣。而整个系统的部署方式和GraphLearn也有一些相似性。
Challenges still exist 在很多业务场景下，图并不是静态不变的而是呈现出一种时间特征，一些点在$t_0$时加入，之后在$t_1$时删除。但是时序图GNN（TGNN）仍然是一个open的问题 7。在近几年GNN的发展中，开放的标准数据集（e.g. OGB 8）有着巨大的贡献，不论是GNN算法还是GNN系统开发，都会将OGB作为数据集和其他baseline比较。然而目前TGNN并没有对应的标准数据集，这样我们很难去比较不同来自不同团队的工作。另一个方面，GNN的收敛性和表达能力是可以被证明的，但是TGNN还是一个相对较新的算法，并没有GNN成熟，缺少基础的理论支撑。
GNN虽然网络层数少，计算资源需求不大，但是图和特征需要占用大量的存储资源，这给GNN部署和落地带来了一些挑战 9。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://Anlarry.github.io/posts/"},{"@type":"ListItem","position":2,"name":"GNN Application in Recommendation","item":"https://Anlarry.github.io/posts/paper-reading/gnn-application-in-recommendation/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"GNN Application in Recommendation","name":"GNN Application in Recommendation","description":"Prelude GNN是最近几年中机器学习较新的一个发展方向，也是学术界和工业界研究的热点话题。相比于node2vec、PageRank，GNN具有更强大的学习能力，能够更好得感知图数据。\nGNN的一个经典应用场景是推荐系统。像淘宝这样的电商平台，大量的用户和商品，自然而然地形成了一个复杂异构图，一个直观想法是，可以利用GNN学习用户的兴趣特征，根据用户的历史浏览推荐用户更感兴趣的商品。而且这种方法的推荐质量也更高。\nPinterest Let\u0026rsquo;s begin with PinSage 1 GNN的开山制作——GCN 2把GNN训练用矩阵乘法来刻画，这样会导致一个问题，每一层每个点都要从他的邻居中读取feature (i.e. 全图训练)，导致GNN的复杂度非常高。\n后来，一个点在他的部分邻居中读取feature（i.e. 图采样）就可以保证模型的准确度，这样就减低了GNN训练的成本。PinSage也是一个基于采样的GNN算法，是一个早期同时也非常经典的GNN算法。和GraphSage相比，最大的不同点在于用random-walk，采样的子图的质量会好一些。\n比如上图就是一个两层的GNN, 在采样完成后我们会得到很多个子图（i.e. mini-batch）并喂给网络训练。每层网络都是如下图所示的图卷积操作，通过聚合特征（e.g. 取平均，求和）再通过一层神经网络得到一下层的输入。将多层图卷积stack到一起就构成了整个GNN，一般只有2-3层。\n然而这和推荐系统有什么关系呢？在Pinterest中，有很多pin（类似淘宝的商品，在推荐系统中也称之为item），每个item会包含图片、文本信息，将图片和文本的embedding拼接到一起得到item的初始特征向量。在训练完GNN之后，通过推理得到每个item的嵌入向量，之后推荐系统就可以根据embedding向用户推荐新的item。\nRecoommandation System at Pinterest 到推荐系统中，我们会认为每个item的embeddin向量是通过一个黑箱过程（i.e. PinSage），是提前已知的 3。我们的问题是，知道用户点击item历史序列$\\mathcal{A}=\\{a_1, a_2, \u0026hellip;\\}$，我们想从$\\mathcal{P}=\\{p_1, p_2, \u0026hellip;\\}$中预测下一个用户可能感兴趣的商品。\n我们可以想到一些很直观的方法，比如历史的item求平均，还可以按时间给一个权重。但是这样的方法把用户的embedding限制在一个有限的向量里面，效果不是很好，因为一个用户兴趣面是是很广的，对不同种类的商品有不同的喜好。PinnerSage 3 会给用户维护一个不限大小的embedding，不同的种类的喜好都对应到一个嵌入向量。\n然而还有另外一个问题——用户的历史序列会很长，计算的复杂度会非常大，影响推荐的系统的延迟。PinnerSage采用的方法是，通过聚类，每个商品都可以用一个类中心来表示。在推荐的时候，根据用户的action再给这些类中心一个重要性评分。最后用最重要的类（e.g. top 3）去预测下一个item。\n总结一下PinnerSage的推荐过程分为3步：\n用户历史动作序列聚类 得到类中心嵌入向量（Medoid based） 根据time decay计算类的重要度 找哪一个item最合适是一个索引过程，一般会通过cache加速查询。为了兼顾用户的长期兴趣和系统延迟，PinnerSage将任务分成两类，daily和online。在每天PinnerSage都会把用户的历史动作进行聚类的，得到类中心和重要度，这样可以保证和用户兴趣。而在online推荐的时候，只会对用最新的动作在已有的类上refine，降低推荐的延迟。\nAliGraph 4 图数据规模的增大给GNN训练带来新的挑战，在工业界，图的规模会非常大，点和边往往是billion量级的。Ali坐拥最大的电商平台之一，大规模图神经网络既有真实的场景，同时也是一个亟须解决的问题。\nAliGraph是ali的一个图神经网络框架。从系统的角度来看，AliGraph需要为上层用户提供应有的支撑，使得上层算法和应用开发可以仅仅关注自身的逻辑，而不用担心资源如何分配的问题。\n面对超大规模的图，AliGraph通过parition把图分布式存在集群上，解决了存储的问题。同时通过维护cache，尽可能减少partition之间的通讯开销。同时AliGraph还需要实现常见的算子来支持上层GNN应用。AliGraph也进行了开源，GraphLearn 5，同时支持推理和训练任务。\nGNN训练基本都是大同小异，而推理任务是部署和落地中主要workload，而且大部分的计算资源也是花在推理上。考虑到真实场景下，图一般是动态（e.g. 新用户的加入），GraphLearn后来增加了Dynamic Graph Service (DGS)来支持动态图的推理任务。\n整个系统通过微服务的方式运行，每个模块被拆到subservices，模块之间都通过队列连接起来（e.g. kafka）。\n美团广告推荐6 可以发现在和Pinterest的推荐系统其实在设计上有很多共同点。\n美团在推荐任务中有两个观察点：数据样本稀疏，广告投放后点击量非常少；用户兴趣存在时空特点，在不同时间点和地点用户有不同的兴趣点。通过异构图构建、注意力机制等方法解决的推荐不准确的问题。\n整个推荐也分成online任务，保证请求低延迟；以及offline任务，用来更新图节点embedding，学习用户的长期兴趣。而整个系统的部署方式和GraphLearn也有一些相似性。\nChallenges still exist 在很多业务场景下，图并不是静态不变的而是呈现出一种时间特征，一些点在$t_0$时加入，之后在$t_1$时删除。但是时序图GNN（TGNN）仍然是一个open的问题 7。在近几年GNN的发展中，开放的标准数据集（e.g. OGB 8）有着巨大的贡献，不论是GNN算法还是GNN系统开发，都会将OGB作为数据集和其他baseline比较。然而目前TGNN并没有对应的标准数据集，这样我们很难去比较不同来自不同团队的工作。另一个方面，GNN的收敛性和表达能力是可以被证明的，但是TGNN还是一个相对较新的算法，并没有GNN成熟，缺少基础的理论支撑。\nGNN虽然网络层数少，计算资源需求不大，但是图和特征需要占用大量的存储资源，这给GNN部署和落地带来了一些挑战 9。","keywords":["GNN","Recommendation"],"articleBody":"Prelude GNN是最近几年中机器学习较新的一个发展方向，也是学术界和工业界研究的热点话题。相比于node2vec、PageRank，GNN具有更强大的学习能力，能够更好得感知图数据。\nGNN的一个经典应用场景是推荐系统。像淘宝这样的电商平台，大量的用户和商品，自然而然地形成了一个复杂异构图，一个直观想法是，可以利用GNN学习用户的兴趣特征，根据用户的历史浏览推荐用户更感兴趣的商品。而且这种方法的推荐质量也更高。\nPinterest Let’s begin with PinSage 1 GNN的开山制作——GCN 2把GNN训练用矩阵乘法来刻画，这样会导致一个问题，每一层每个点都要从他的邻居中读取feature (i.e. 全图训练)，导致GNN的复杂度非常高。\n后来，一个点在他的部分邻居中读取feature（i.e. 图采样）就可以保证模型的准确度，这样就减低了GNN训练的成本。PinSage也是一个基于采样的GNN算法，是一个早期同时也非常经典的GNN算法。和GraphSage相比，最大的不同点在于用random-walk，采样的子图的质量会好一些。\n比如上图就是一个两层的GNN, 在采样完成后我们会得到很多个子图（i.e. mini-batch）并喂给网络训练。每层网络都是如下图所示的图卷积操作，通过聚合特征（e.g. 取平均，求和）再通过一层神经网络得到一下层的输入。将多层图卷积stack到一起就构成了整个GNN，一般只有2-3层。\n然而这和推荐系统有什么关系呢？在Pinterest中，有很多pin（类似淘宝的商品，在推荐系统中也称之为item），每个item会包含图片、文本信息，将图片和文本的embedding拼接到一起得到item的初始特征向量。在训练完GNN之后，通过推理得到每个item的嵌入向量，之后推荐系统就可以根据embedding向用户推荐新的item。\nRecoommandation System at Pinterest 到推荐系统中，我们会认为每个item的embeddin向量是通过一个黑箱过程（i.e. PinSage），是提前已知的 3。我们的问题是，知道用户点击item历史序列$\\mathcal{A}=\\{a_1, a_2, …\\}$，我们想从$\\mathcal{P}=\\{p_1, p_2, …\\}$中预测下一个用户可能感兴趣的商品。\n我们可以想到一些很直观的方法，比如历史的item求平均，还可以按时间给一个权重。但是这样的方法把用户的embedding限制在一个有限的向量里面，效果不是很好，因为一个用户兴趣面是是很广的，对不同种类的商品有不同的喜好。PinnerSage 3 会给用户维护一个不限大小的embedding，不同的种类的喜好都对应到一个嵌入向量。\n然而还有另外一个问题——用户的历史序列会很长，计算的复杂度会非常大，影响推荐的系统的延迟。PinnerSage采用的方法是，通过聚类，每个商品都可以用一个类中心来表示。在推荐的时候，根据用户的action再给这些类中心一个重要性评分。最后用最重要的类（e.g. top 3）去预测下一个item。\n总结一下PinnerSage的推荐过程分为3步：\n用户历史动作序列聚类 得到类中心嵌入向量（Medoid based） 根据time decay计算类的重要度 找哪一个item最合适是一个索引过程，一般会通过cache加速查询。为了兼顾用户的长期兴趣和系统延迟，PinnerSage将任务分成两类，daily和online。在每天PinnerSage都会把用户的历史动作进行聚类的，得到类中心和重要度，这样可以保证和用户兴趣。而在online推荐的时候，只会对用最新的动作在已有的类上refine，降低推荐的延迟。\nAliGraph 4 图数据规模的增大给GNN训练带来新的挑战，在工业界，图的规模会非常大，点和边往往是billion量级的。Ali坐拥最大的电商平台之一，大规模图神经网络既有真实的场景，同时也是一个亟须解决的问题。\nAliGraph是ali的一个图神经网络框架。从系统的角度来看，AliGraph需要为上层用户提供应有的支撑，使得上层算法和应用开发可以仅仅关注自身的逻辑，而不用担心资源如何分配的问题。\n面对超大规模的图，AliGraph通过parition把图分布式存在集群上，解决了存储的问题。同时通过维护cache，尽可能减少partition之间的通讯开销。同时AliGraph还需要实现常见的算子来支持上层GNN应用。AliGraph也进行了开源，GraphLearn 5，同时支持推理和训练任务。\nGNN训练基本都是大同小异，而推理任务是部署和落地中主要workload，而且大部分的计算资源也是花在推理上。考虑到真实场景下，图一般是动态（e.g. 新用户的加入），GraphLearn后来增加了Dynamic Graph Service (DGS)来支持动态图的推理任务。\n整个系统通过微服务的方式运行，每个模块被拆到subservices，模块之间都通过队列连接起来（e.g. kafka）。\n美团广告推荐6 可以发现在和Pinterest的推荐系统其实在设计上有很多共同点。\n美团在推荐任务中有两个观察点：数据样本稀疏，广告投放后点击量非常少；用户兴趣存在时空特点，在不同时间点和地点用户有不同的兴趣点。通过异构图构建、注意力机制等方法解决的推荐不准确的问题。\n整个推荐也分成online任务，保证请求低延迟；以及offline任务，用来更新图节点embedding，学习用户的长期兴趣。而整个系统的部署方式和GraphLearn也有一些相似性。\nChallenges still exist 在很多业务场景下，图并不是静态不变的而是呈现出一种时间特征，一些点在$t_0$时加入，之后在$t_1$时删除。但是时序图GNN（TGNN）仍然是一个open的问题 7。在近几年GNN的发展中，开放的标准数据集（e.g. OGB 8）有着巨大的贡献，不论是GNN算法还是GNN系统开发，都会将OGB作为数据集和其他baseline比较。然而目前TGNN并没有对应的标准数据集，这样我们很难去比较不同来自不同团队的工作。另一个方面，GNN的收敛性和表达能力是可以被证明的，但是TGNN还是一个相对较新的算法，并没有GNN成熟，缺少基础的理论支撑。\nGNN虽然网络层数少，计算资源需求不大，但是图和特征需要占用大量的存储资源，这给GNN部署和落地带来了一些挑战 9。\n这就产生了一个新的问题，在部署GNN时我们希望可以降低存储的开销，Serving graph compression for GNNs。Model compression、Coreset selection、 Dataset distillation是目前的一些常见的方法。\nGNN如今已经在推荐系统中有了一席之地。之后，随着GNN应用逐渐扩大、GNN算法的改进、软件环境的变化，也许会产生新的设计，but no one can predict。\nGraph Convolutional Neural Networks for Web-Scale Recommender Systems ↩︎\nSEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS ↩︎\nPinnerSage: Multi-Modal User Embedding Framework for Recommendations at Pinterest ↩︎ ↩︎\nAliGraph: A Comprehensive Graph Neural Network Platform ↩︎\nGraphLearn ↩︎\n大规模异构图召回在美团到店推荐广告的应用 ↩︎\nGraph Neural Networks for temporal graphs: State of the art, open challenges, and opportunities ↩︎\nOpen Graph Benchmark: Datasets for Machine Learning on Graphs ↩︎\nServing Graph Compression for Graph Neural Networks ↩︎\n","wordCount":"146","inLanguage":"en","image":"https://Anlarry.github.io/2023-03-17-19-45-23.png","datePublished":"2023-03-18T00:00:00Z","dateModified":"2023-03-18T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://Anlarry.github.io/posts/paper-reading/gnn-application-in-recommendation/"},"publisher":{"@type":"Organization","name":"Hi-^_^","logo":{"@type":"ImageObject","url":"https://Anlarry.github.io/icon/ai512.png"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://Anlarry.github.io/ accesskey=h title="Jiali's Blog (Alt + H)"><img src=/icon/ai512.png alt=logo aria-label=logo height=35>Jiali's Blog</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://Anlarry.github.io/archives/ title="Archives 📦"><span>Archives 📦</span></a></li><li><a href=https://Anlarry.github.io/categories/ title="Categories 📂"><span>Categories 📂</span></a></li><li><a href=https://Anlarry.github.io/tags/ title="Tags 🏷"><span>Tags 🏷</span></a></li><li><a href=https://Anlarry.github.io/book_reading/ title="Book-Reading 📚"><span>Book-Reading 📚</span></a></li><li><a href=https://Anlarry.github.io/links/ title="Links 🔗"><span>Links 🔗</span></a></li><li><a href=https://Anlarry.github.io/search/ title="Search 🔍 (Alt + /)" accesskey=/><span>Search 🔍</span></a></li></ul></nav></header><main class=main><link rel=stylesheet href=https://Anlarry.github.io/sass/single-main.css><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://Anlarry.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://Anlarry.github.io/posts/>Posts</a></div><h1 class=post-title>GNN Application in Recommendation</h1><div class=post-meta>📝 March 18, 2023&nbsp;·&nbsp;⌛ 1 min&nbsp;|&nbsp;
<a href=https://github.com/Anlarry/Anlarry.github.io/issues rel="noopener noreferrer" target=_blank>Suggest Changes and Comment</a></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><ul><li><a href=#prelude aria-label=Prelude>Prelude</a></li></ul><li><a href=#pinterest aria-label=Pinterest>Pinterest</a><ul><li><a href=#lets-begin-with-pinsage-1 aria-label="Let&amp;rsquo;s begin with PinSage 1">Let&rsquo;s begin with PinSage <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></a></li><li><a href=#recoommandation-system-at-pinterest aria-label="Recoommandation System at Pinterest">Recoommandation System at Pinterest</a></li></ul></li><li><a href=#aligraph-3 aria-label="AliGraph 4">AliGraph <sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup></a></li><li><a href=#%e7%be%8e%e5%9b%a2%e5%b9%bf%e5%91%8a%e6%8e%a8%e8%8d%904 aria-label=美团广告推荐6>美团广告推荐<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup></a></li><li><a href=#challenges-still-exist aria-label="Challenges still exist">Challenges still exist</a></li></ul></div></details></div><div class=post-content><h3 id=prelude>Prelude<a hidden class=anchor aria-hidden=true href=#prelude>#</a></h3><p>GNN是最近几年中机器学习较新的一个发展方向，也是学术界和工业界研究的热点话题。相比于node2vec、PageRank，GNN具有更强大的学习能力，能够更好得感知图数据。</p><p>GNN的一个经典应用场景是推荐系统。像淘宝这样的电商平台，大量的用户和商品，自然而然地形成了一个复杂异构图，一个直观想法是，可以利用GNN学习用户的兴趣特征，根据用户的历史浏览推荐用户更感兴趣的商品。而且这种方法的推荐质量也更高。</p><h2 id=pinterest>Pinterest<a hidden class=anchor aria-hidden=true href=#pinterest>#</a></h2><h3 id=lets-begin-with-pinsage-1>Let&rsquo;s begin with PinSage <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup><a hidden class=anchor aria-hidden=true href=#lets-begin-with-pinsage-1>#</a></h3><p>GNN的开山制作——GCN <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>把GNN训练用矩阵乘法来刻画，这样会导致一个问题，每一层每个点都要从他的邻居中读取feature (i.e. 全图训练)，导致GNN的复杂度非常高。</p><p>后来，一个点在他的部分邻居中读取feature（i.e. 图采样）就可以保证模型的准确度，这样就减低了GNN训练的成本。PinSage也是一个基于采样的GNN算法，是一个早期同时也非常经典的GNN算法。和GraphSage相比，最大的不同点在于用random-walk，采样的子图的质量会好一些。</p><p><img src=2023-03-17-20-42-27.png#center alt></p><p>比如上图就是一个两层的GNN, 在采样完成后我们会得到很多个子图（i.e. mini-batch）并喂给网络训练。每层网络都是如下图所示的图卷积操作，通过聚合特征（e.g. 取平均，求和）再通过一层神经网络得到一下层的输入。将多层图卷积stack到一起就构成了整个GNN，一般只有2-3层。</p><p><img src=2023-03-17-20-54-29.png#center-medium alt></p><p>然而这和推荐系统有什么关系呢？在Pinterest中，有很多<em>pin</em>（类似淘宝的商品，在推荐系统中也称之为<em>item</em>），每个item会包含图片、文本信息，将图片和文本的embedding拼接到一起得到item的初始特征向量。在训练完GNN之后，通过推理得到每个item的嵌入向量，之后推荐系统就可以根据embedding向用户推荐新的item。</p><h3 id=recoommandation-system-at-pinterest>Recoommandation System at Pinterest<a hidden class=anchor aria-hidden=true href=#recoommandation-system-at-pinterest>#</a></h3><p>到推荐系统中，我们会认为每个item的embeddin向量是通过一个黑箱过程（i.e. PinSage），是提前已知的 <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>。我们的问题是，知道用户点击item历史序列$\mathcal{A}=\{a_1, a_2, &mldr;\}$，我们想从$\mathcal{P}=\{p_1, p_2, &mldr;\}$中预测下一个用户可能感兴趣的商品。</p><p>我们可以想到一些很直观的方法，比如历史的item求平均，还可以按时间给一个权重。但是这样的方法把用户的embedding限制在一个有限的向量里面，效果不是很好，因为一个用户兴趣面是是很广的，对不同种类的商品有不同的喜好。PinnerSage <sup id=fnref1:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> 会给用户维护一个不限大小的embedding，不同的种类的喜好都对应到一个嵌入向量。</p><p>然而还有另外一个问题——用户的历史序列会很长，计算的复杂度会非常大，影响推荐的系统的延迟。PinnerSage采用的方法是，通过聚类，每个商品都可以用一个类中心来表示。在推荐的时候，根据用户的action再给这些类中心一个重要性评分。最后用最重要的类（e.g. top 3）去预测下一个item。</p><p>总结一下PinnerSage的推荐过程分为3步：</p><ul><li>用户历史动作序列聚类</li><li>得到类中心嵌入向量（Medoid based）</li></ul><p><img src=2023-03-17-22-25-04.png#center-large alt></p><ul><li>根据time decay计算类的重要度</li></ul><p><img src=2023-03-17-21-18-03.png#center-medium alt></p><p>找哪一个item最合适是一个索引过程，一般会通过cache加速查询。为了兼顾用户的长期兴趣和系统延迟，PinnerSage将任务分成两类，daily和online。在每天PinnerSage都会把用户的历史动作进行聚类的，得到类中心和重要度，这样可以保证和用户兴趣。而在online推荐的时候，只会对用最新的动作在已有的类上refine，降低推荐的延迟。</p><h2 id=aligraph-3>AliGraph <sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup><a hidden class=anchor aria-hidden=true href=#aligraph-3>#</a></h2><p>图数据规模的增大给GNN训练带来新的挑战，在工业界，图的规模会非常大，点和边往往是billion量级的。Ali坐拥最大的电商平台之一，大规模图神经网络既有真实的场景，同时也是一个亟须解决的问题。</p><p>AliGraph是ali的一个图神经网络框架。从系统的角度来看，AliGraph需要为上层用户提供应有的支撑，使得上层算法和应用开发可以仅仅关注自身的逻辑，而不用担心资源如何分配的问题。</p><p><img src=2023-03-17-22-43-30.png#center-medium alt></p><p>面对超大规模的图，AliGraph通过parition把图分布式存在集群上，解决了存储的问题。同时通过维护cache，尽可能减少partition之间的通讯开销。同时AliGraph还需要实现常见的算子来支持上层GNN应用。AliGraph也进行了开源，GraphLearn <sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>，同时支持推理和训练任务。</p><p><img src=2023-03-17-23-39-15.png alt></p><p>GNN训练基本都是大同小异，而推理任务是部署和落地中主要workload，而且大部分的计算资源也是花在推理上。考虑到真实场景下，图一般是动态（e.g. 新用户的加入），GraphLearn后来增加了Dynamic Graph Service (DGS)来支持动态图的推理任务。</p><p><img src=2023-03-17-23-45-57.png alt></p><p>整个系统通过微服务的方式运行，每个模块被拆到subservices，模块之间都通过队列连接起来（e.g. kafka）。</p><h2 id=美团广告推荐4>美团广告推荐<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup><a hidden class=anchor aria-hidden=true href=#美团广告推荐4>#</a></h2><blockquote><p>可以发现在和Pinterest的推荐系统其实在设计上有很多共同点。</p></blockquote><p>美团在推荐任务中有两个观察点：<strong>数据样本稀疏</strong>，广告投放后点击量非常少；<strong>用户兴趣存在时空特点</strong>，在不同时间点和地点用户有不同的兴趣点。通过异构图构建、注意力机制等方法解决的推荐不准确的问题。</p><p><img src=2023-03-18-00-04-16.png alt></p><p>整个推荐也分成online任务，保证请求低延迟；以及offline任务，用来更新图节点embedding，学习用户的长期兴趣。而整个系统的部署方式和GraphLearn也有一些相似性。</p><h2 id=challenges-still-exist>Challenges still exist<a hidden class=anchor aria-hidden=true href=#challenges-still-exist>#</a></h2><p>在很多业务场景下，图并不是静态不变的而是呈现出一种时间特征，一些点在$t_0$时加入，之后在$t_1$时删除。但是时序图GNN（TGNN）仍然是一个open的问题 <sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup>。在近几年GNN的发展中，开放的标准数据集（e.g. OGB <sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup>）有着巨大的贡献，不论是GNN算法还是GNN系统开发，都会将OGB作为数据集和其他baseline比较。然而目前TGNN并没有对应的标准数据集，这样我们很难去比较不同来自不同团队的工作。另一个方面，GNN的收敛性和表达能力是可以被证明的，但是TGNN还是一个相对较新的算法，并没有GNN成熟，缺少基础的理论支撑。</p><p>GNN虽然网络层数少，计算资源需求不大，但是图和特征需要占用大量的存储资源，这给GNN部署和落地带来了一些挑战 <sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup>。</p><p><img src=2023-03-18-13-38-30.png#center-large alt></p><p>这就产生了一个新的问题，在部署GNN时我们希望可以降低存储的开销，Serving graph compression for GNNs。Model compression、Coreset selection、 Dataset distillation是目前的一些常见的方法。</p><p>GNN如今已经在推荐系统中有了一席之地。之后，随着GNN应用逐渐扩大、GNN算法的改进、软件环境的变化，也许会产生新的设计，but no one can predict。</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://dl.acm.org/doi/pdf/10.1145/3219819.3219890>Graph Convolutional Neural Networks for Web-Scale Recommender Systems</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href=https://arxiv.org/pdf/1609.02907.pdf>SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p><a href=https://dl.acm.org/doi/pdf/10.1145/3394486.3403280>PinnerSage: Multi-Modal User Embedding Framework for Recommendations at Pinterest</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p><a href=https://dl.acm.org/doi/pdf/10.14778/3352063.3352127>AliGraph: A Comprehensive Graph Neural Network Platform</a>&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p><a href=https://github.com/alibaba/graph-learn>GraphLearn</a>&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p><a href=https://tech.meituan.com/2022/11/24/application-of-large-scale-heterogeneous-graph-in-meituan-recommended-ads.html>大规模异构图召回在美团到店推荐广告的应用</a>&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7><p><a href=https://arxiv.org/pdf/2302.01018.pdf>Graph Neural Networks for temporal graphs: State of the art, open challenges, and opportunities</a>&#160;<a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8><p><a href=https://arxiv.org/abs/2005.00687>Open Graph Benchmark: Datasets for Machine Learning on Graphs</a>&#160;<a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:9><p><a href="https://openreview.net/pdf?id=T-qVtA3pAxG">Serving Graph Compression for Graph Neural Networks</a>&#160;<a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://Anlarry.github.io/tags/gnn/>GNN</a></li><li><a href=https://Anlarry.github.io/tags/recommendation/>Recommendation</a></li></ul><nav class=paginav><a class=next href=https://Anlarry.github.io/posts/tool/xelatexunicode/><span class=title>Next Page »</span><br><span>Unicode with xelatex</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://Anlarry.github.io/>Jiali Wang</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="📄";function s(){t.innerText="✔",setTimeout(()=>{t.innerText="📄"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>