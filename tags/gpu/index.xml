<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>GPU on Hi-^_^</title>
    <link>https://Anlarry.github.io/tags/gpu/</link>
    <description>Recent content in GPU on Hi-^_^</description>
    <generator>Hugo -- 0.147.5</generator>
    <language>en</language>
    <copyright>Jiali Wang</copyright>
    <lastBuildDate>Sun, 14 Nov 2021 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://Anlarry.github.io/tags/gpu/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>NextDoor</title>
      <link>https://Anlarry.github.io/posts/paper-reading/nextdoor/</link>
      <pubDate>Sun, 14 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://Anlarry.github.io/posts/paper-reading/nextdoor/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;基本各种东西都可以用图来表示，也就促成了GNN的快速发展。然而很多图都有这样的特点，一些节点的度数很高然而大多数节点的度数很低。&lt;/p&gt;
&lt;p&gt;训练时要用邻居更新当前节点，不能将整个图全部拿来训练，因此一般采用graph sampling+mini batch，比如，给$n$个节点，每个节点对邻居采样后在进行训练。&lt;/p&gt;
&lt;p&gt;然而产生了新的问题，采样在整个训练过程中占了很多时间。&lt;/p&gt;
&lt;p&gt;./image-20211114114025150.png#center-small#center-medium
&lt;img class=&#34;img-fluid&#34; src=&#34;./image-20211114114025150.png#center-small#center-medium&#34; alt=&#39;&#39; /&gt;
&lt;/p&gt;
&lt;p&gt;因此，大家也会去想用GPU加速采样，但是naive的方法并不能很好的利用GPU。&lt;/p&gt;
&lt;h2 id=&#34;transit-parallelism&#34;&gt;&amp;ldquo;Transit-Parallelism&amp;rdquo;&lt;/h2&gt;
&lt;p&gt;作者首先对采样进行了抽象，采样从一节点开始，扩展出新的节点加入sample，再从新的节点扩展$\cdots$，每次遍历邻居扩展新节点的节点，作者把它叫做&amp;quot;transit vertices&amp;quot;&lt;/p&gt;
&lt;p&gt;将采样分成两类：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Individual Transit Sampling&lt;/strong&gt;，这个是按节点来的，每个transit节点从邻居中采样一定数量的节点&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Collective Transit Sampling&lt;/strong&gt;，这个是按层来，每一层从所有transit节点的邻居中采样一定数量的节点&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;image-20211114114804228.png#center
&lt;img class=&#34;img-fluid&#34; src=&#34;image-20211114114804228.png#center&#34; alt=&#39;image-20211114114804228&#39; /&gt;
&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;CUDA &amp;amp; GPU&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;一个CUDA程序被划分给很多blocks of threads完成并行，而GPU又由很多StreamingMultiprocessors (SMs)构成，每个block被放到SM上执行&lt;/p&gt;
&lt;p&gt;automatic-scalability.png#center
&lt;img class=&#34;img-fluid&#34; src=&#34;automatic-scalability.png#center&#34; alt=&#39;Automatic Scalability.&#39; /&gt;
&lt;/p&gt;
&lt;p&gt;一个block的线程数是有限的，但是相同的大小block可以被组织成grid，于是kernel(a c++ function)就可以用grid里面所有的线程。
$$
thread \xrightarrow[]{array} block \xrightarrow[]{array} grid
$$
众所周知，内存层次结构，GPU当然也有：&lt;/p&gt;
&lt;p&gt;memory-hierarchy.png#center
&lt;img class=&#34;img-fluid&#34; src=&#34;memory-hierarchy.png#center&#34; alt=&#39;Memory Hierarchy.&#39; /&gt;
&lt;/p&gt;
&lt;p&gt;前面已经提到了block会在SM上执行，在物理实现时会用到SIMT(Single-Instruction, Multiple-Thread)。multiprocessor用warp(a group of 32 threads)来管理线程。&lt;/p&gt;
&lt;p&gt;warp中的线程都从相同的起点开始，但是每个线程都有自己的pc，寄存器状态也可能不一样。而且一个warp的thread执行的指令还是相同的。&lt;/p&gt;
&lt;p&gt;如果就是普通的没有控制流的代码，大家就一起执行。那遇到分支怎么办，每个线程可能有不同的路径。这时就会变成串行。warp去执行每个分支路径，不在路径上的thread就等着。这就可能会很影响并行，也就是Branch divergence。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Sample-Parallelism&lt;/strong&gt;，对于Individual Transit Sampling可以将每一对sample和transit分配$m_i$个线程，每个sample放到一个block里。对于Collective Transit Sampling，需要先把所有的邻居存到global memory里，再采样。&lt;/p&gt;
&lt;p&gt;image-20211114120722353.png#center-medium
&lt;img class=&#34;img-fluid&#34; src=&#34;image-20211114120722353.png#center-medium&#34; alt=&#39;image-20211114120722353&#39; /&gt;
&lt;/p&gt;
&lt;p&gt;在采样时，邻居多的节点计算的时间就会更久。如果同一个warp里面的两个thread被分给两个邻居数量不同的transit，就会有divergence。而且，图得存在gobal memory，shared memory利用不充分。&lt;/p&gt;
&lt;p&gt;但是如果是按transit划分，局部性就会更好，按照工作量需求分配线程数量。是不是有点像倒排索引 :-)&lt;/p&gt;
&lt;p&gt;image-20211114121110605.png#center-medium
&lt;img class=&#34;img-fluid&#34; src=&#34;image-20211114121110605.png#center-medium&#34; alt=&#39;image-20211114121110605&#39; /&gt;
&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
